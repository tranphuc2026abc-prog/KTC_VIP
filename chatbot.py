# streamlit_app.py
import os
import glob
import time
from typing import List, Optional

import streamlit as st
from pypdf import PdfReader

# AI / RAG libs
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# Translator (HuggingFace)
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# Groq client for LLM streaming (as in your original)
from groq import Groq

# --- 0. C·∫§U H√åNH CHUNG ---
st.set_page_config(
    page_title="KTC Assistant - Tr·ª£ l√Ω Tin h·ªçc 2025",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

CONSTANTS = {
    "MODEL_NAME": 'llama-3.1-8b-instant',
    "PDF_DIR": "./PDF_KNOWLEDGE",
    "VECTOR_STORE_PATH": "./faiss_db_index",
    "LOGO_PATH": "LOGO.jpg",
    # M√¥ h√¨nh embedding h·ªó tr·ª£ nhi·ªÅu ng√¥n ng·ªØ, t·ªët cho vi/en cross-lingual
    "EMBEDDING_MODEL": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    # D·ªãch Vi -> En (c√≥ th·ªÉ ƒë·ªïi sang model kh√°c n·∫øu mu·ªën)
    "TRANSLATION_MODEL": "Helsinki-NLP/opus-mt-vi-en",
    "CHUNK_SIZE": 800,
    "CHUNK_OVERLAP": 150,
    "TOP_K": 3,
}

# --- 1. CSS / Giao di·ªán c∆° b·∫£n ---
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap');
    html, body, [class*="css"] { font-family: 'Roboto', sans-serif; }
    .stApp {background-color: #f8f9fa;}
    [data-testid="stSidebar"] { background-color: #ffffff; border-right: 1px solid #e0e0e0; }
    .gradient-text {
        background: linear-gradient(90deg, #0052cc, #00c6ff);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-weight: 800;
        font-size: 2.2rem;
        text-align: center;
        padding: 10px 0;
    }
    .source-box {
        font-size: 0.85rem; color: #444; background: #f1f1f1;
        padding: 8px; border-radius: 6px; margin-top: 8px; border-left: 3px solid #0284c7;
    }
</style>
""", unsafe_allow_html=True)

# --- 2. CACHE / T√ÄI NGUY√äN D√ôNG CHUNG ---
@st.cache_resource(show_spinner=False)
def get_groq_client():
    """Load Groq client 1 l·∫ßn."""
    try:
        api_key = st.secrets["GROQ_API_KEY"]
        return Groq(api_key=api_key)
    except Exception:
        return None

@st.cache_resource(show_spinner=False)
def get_embeddings():
    """T·∫°o HuggingFaceEmbeddings 1 l·∫ßn v√† cache l·∫°i."""
    return HuggingFaceEmbeddings(model_name=CONSTANTS["EMBEDDING_MODEL"])

@st.cache_resource(show_spinner=False)
def get_translator():
    """T·∫£i model d·ªãch (vi->en) m·ªôt l·∫ßn. Tr·∫£ v·ªÅ pipeline d·ªãch."""
    model_name = CONSTANTS["TRANSLATION_MODEL"]
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    return pipeline("translation", model=model, tokenizer=tokenizer, src_lang="vi", tgt_lang="en")

# --- 3. CLASS KnowledgeBase (ƒë√£ c·∫£i ti·∫øn) ---
class KnowledgeBase:
    """Qu·∫£n l√Ω ƒë·ªçc file, t√°ch vƒÉn b·∫£n, t·∫°o/ load vector DB."""
    def __init__(self, embeddings):
        self.embeddings = embeddings

    def load_documents(self) -> List[Document]:
        """ƒê·ªçc t·∫•t c·∫£ PDF trong th∆∞ m·ª•c v√† tr·∫£ v·ªÅ danh s√°ch Document (langchain)."""
        if not os.path.exists(CONSTANTS["PDF_DIR"]):
            os.makedirs(CONSTANTS["PDF_DIR"])
            return []

        pdf_files = glob.glob(os.path.join(CONSTANTS["PDF_DIR"], "*.pdf"))
        documents: List[Document] = []
        for pdf_path in pdf_files:
            try:
                reader = PdfReader(pdf_path)
                file_name = os.path.basename(pdf_path)
                for i, page in enumerate(reader.pages):
                    text = page.extract_text()
                    if text and text.strip():
                        documents.append(Document(
                            page_content=text,
                            metadata={"source": file_name, "page": i + 1}
                        ))
            except Exception as e:
                st.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file {pdf_path}: {e}")
        return documents

    def build_or_load_vector_db(self, force_rebuild: bool = False) -> Optional[FAISS]:
        """
        N·∫øu ƒë√£ c√≥ l∆∞u tr√™n disk -> load, n·∫øu kh√¥ng -> build m·ªõi.
        D√πng cache_resource ·ªü m·ª©c g·ªçi h√†m n√†y ƒë·ªÉ tr√°nh build l·∫°i nhi·ªÅu l·∫ßn.
        """
        path = CONSTANTS["VECTOR_STORE_PATH"]
        # Try load
        if os.path.exists(path) and not force_rebuild:
            try:
                return FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization=True)
            except Exception:
                # n·∫øu load l·ªói --> build l·∫°i
                pass

        # Build m·ªõi
        docs = self.load_documents()
        if not docs:
            return None

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CONSTANTS["CHUNK_SIZE"],
            chunk_overlap=CONSTANTS["CHUNK_OVERLAP"]
        )
        splits = text_splitter.split_documents(docs)
        if not splits:
            return None

        vector_db = FAISS.from_documents(splits, self.embeddings)
        # L∆∞u xu·ªëng disk
        try:
            os.makedirs(path, exist_ok=True)
            vector_db.save_local(path)
        except Exception as e:
            st.warning(f"Kh√¥ng l∆∞u ƒë∆∞·ª£c vector DB: {e}")
        return vector_db

# --- 4. C√°c h√†m ti·ªán √≠ch (translate + search + format) ---
def translate_vi_to_en(translator_pipeline, text: str) -> str:
    """D·ªãch ti·∫øng Vi·ªát sang ti·∫øng Anh. Tr·∫£ v·ªÅ chu·ªói ti·∫øng Anh."""
    if not text or not translator_pipeline:
        return text
    try:
        result = translator_pipeline(text, max_length=512)
        # pipeline tr·∫£ dict ho·∫∑c list dict
        if isinstance(result, list):
            return result[0]["translation_text"]
        return result.get("translation_text", text)
    except Exception:
        # n·∫øu d·ªãch l·ªói -> fallback tr·∫£ v·ªÅ text g·ªëc
        return text

def retrieve_context(vector_db: FAISS, query_en: str, k: int = CONSTANTS["TOP_K"]):
    """T√¨m ki·∫øm t∆∞∆°ng t·ª± (similarity search) tr√™n vector DB b·∫±ng query ti·∫øng Anh.
       Tr·∫£ v·ªÅ context (chu·ªói) v√† list ngu·ªìn."""
    if not vector_db or not query_en:
        return "", []
    try:
        docs = vector_db.similarity_search(query_en, k=k)
        context_parts = []
        sources = []
        for d in docs:
            txt = d.page_content.strip()
            meta = d.metadata or {}
            src = f"{meta.get('source', 'unknown')} (Tr. {meta.get('page', '?')})"
            context_parts.append(f"[TR√çCH]: {txt}")
            sources.append(src)
        context_text = "\n\n".join(context_parts)
        return context_text, sources
    except Exception as e:
        st.warning(f"L·ªói khi truy v·∫•n vector DB: {e}")
        return "", []

def build_system_prompt(context_en: str) -> str:
    """T·∫°o system prompt cho LLM: d·ªØ li·ªáu ng·ªØ c·∫£nh ·ªü d·∫°ng ti·∫øng Anh,
       y√™u c·∫ßu LLM tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."""
    system = f"""
B·∫°n l√† tr·ª£ l√Ω ·∫£o KTC, chuy√™n gia m√¥n Tin h·ªçc theo ch∆∞∆°ng tr√¨nh GDPT 2018.
NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi ng∆∞·ªùi d√πng d·ª±a tr√™n ph·∫ßn [TH√îNG TIN T√ÄI LI·ªÜU] b√™n d∆∞·ªõi.
QUY T·∫ÆC:
1) Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong [TH√îNG TIN T√ÄI LI·ªÜU]. N·∫øu kh√¥ng t√¨m th·∫•y, n√≥i r√µ: "SGK hi·ªán ch∆∞a ƒë·ªÅ c·∫≠p v·∫•n ƒë·ªÅ n√†y."
2) Vi·∫øt b·∫±ng ti·∫øng Vi·ªát chu·∫©n, th√¢n thi·ªán, s∆∞ ph·∫°m, ph√π h·ª£p h·ªçc sinh.
3) N·∫øu tr√≠ch d·∫´n t√†i li·ªáu, li·ªát k√™ ngu·ªìn (t√™n file v√† s·ªë trang).
4) Tr·∫£ l·ªùi ng·∫Øn g·ªçn, c√≥ c·∫•u tr√∫c: ti√™u ƒë·ªÅ in ƒë·∫≠m, c√°c b∆∞·ªõc / g·∫°ch ƒë·∫ßu d√≤ng khi c·∫ßn.
[TH√îNG TIN T√ÄI LI·ªÜU - ENGLISH]:
{context_en}
    """
    return system

# --- 5. Kh·ªüi t·∫°o t√†i nguy√™n (cached) ---
groq_client = get_groq_client()
if groq_client is None:
    st.error("‚ö†Ô∏è L·ªói: Ch∆∞a c·∫•u h√¨nh GROQ_API_KEY trong secrets.")
    st.stop()

embeddings = get_embeddings()
translator = get_translator()
kb = KnowledgeBase(embeddings)

# Load / Build vector DB - NOTE: d√πng cached h√†m ·ªü tr√™n ƒë·ªÉ tr√°nh build l·∫°i li√™n t·ª•c
if "vector_db" not in st.session_state:
    with st.spinner("üîÑ Kh·ªüi t·∫°o h·ªá tri th·ª©c..."):
        st.session_state.vector_db = kb.build_or_load_vector_db(force_rebuild=False)

# --- 6. Sidebar (Control) ---
with st.sidebar:
    if os.path.exists(CONSTANTS["LOGO_PATH"]):
        st.image(CONSTANTS["LOGO_PATH"], use_container_width=True)
    st.title("‚öôÔ∏è Control Panel")
    status_color = "green" if st.session_state.vector_db else "red"
    status_text = "ƒê√£ n·∫°p ki·∫øn th·ª©c" if st.session_state.vector_db else "Ch∆∞a c√≥ d·ªØ li·ªáu"
    st.markdown(f"**Tr·∫°ng th√°i:** <span style='color:{status_color}'>‚óè {status_text}</span>", unsafe_allow_html=True)
    st.markdown("---")
    if st.button("üîÑ Rebuild Vector DB (ƒë·ªçc l·∫°i PDF)"):
        with st.spinner("ƒêang build l·∫°i vector DB ‚Äî c√≥ th·ªÉ m·∫•t v√†i ph√∫t..."):
            st.session_state.vector_db = kb.build_or_load_vector_db(force_rebuild=True)
        st.success("‚úÖ ƒê√£ rebuild xong.")
        time.sleep(0.5)
        st.experimental_rerun()

    if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ Chat"):
        st.session_state.messages = []
        st.experimental_rerun()

    st.markdown("""
    <div style="background:#f8f9fa; padding:12px; border-radius:8px; border:1px dashed #ccc; margin-top:10px;">
        <div style="font-weight:bold; color:#0052cc;">üöÄ D·ª∞ √ÅN KHKT 2025-2026</div>
        <div style="font-size:0.9rem;">GVHD: <b>Th·∫ßy Nguy·ªÖn Th·∫ø Khanh</b></div>
        <div style="font-size:0.9rem;">H·ªçc sinh: <b>B√πi T√° T√πng - Cao S·ªπ B·∫£o Chung</b></div>
    </div>
    """, unsafe_allow_html=True)

# --- 7. Session state cho messages (chat history) ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Xin ch√†o! T√¥i l√† **KTC AI**. H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ Tin h·ªçc trong SGK."}
    ]

# --- 8. MAIN UI: hi·ªÉn th·ªã chat v√† x·ª≠ l√Ω input ---
col1, col2, col3 = st.columns([1, 8, 1])
with col2:
    st.markdown('<h1 class="gradient-text">TR·ª¢ L√ù ·∫¢O TIN H·ªåC KTC</h1>', unsafe_allow_html=True)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        role = msg.get("role", "assistant")
        avatar = "üßë‚Äçüéì" if role == "user" else "ü§ñ"
        with st.chat_message(role, avatar=avatar):
            st.markdown(msg["content"], unsafe_allow_html=True)

    # Input
    prompt = st.chat_input("B·∫°n mu·ªën t√¨m hi·ªÉu g√¨ v·ªÅ Tin h·ªçc? (g√µ ti·∫øng Vi·ªát)")

    if prompt:
        # 1) Append user message & show ngay
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(prompt)

        # 2) Preprocess: d·ªãch sang ti·∫øng Anh ƒë·ªÉ truy v·∫•n vector DB
        with st.spinner("üîé ƒêang t√¨m ki·∫øm th√¥ng tin li√™n quan..."):
            # D·ªãch c√¢u h·ªèi Vi -> En ƒë·ªÉ search
            query_en = translate_vi_to_en(translator, prompt)
            context_en, sources = retrieve_context(st.session_state.vector_db, query_en, k=CONSTANTS["TOP_K"])

        # 3) Build system prompt (context b·∫±ng ti·∫øng Anh) v√† g·ªçi LLM streaming
        system_prompt = build_system_prompt(context_en)

        # Placeholder cho streaming tr·∫£ v·ªÅ
        with st.chat_message("assistant", avatar="ü§ñ"):
            message_placeholder = st.empty()
            full_response = ""

            try:
                # Chu·∫©n b·ªã messages: system + recent history (kho·∫£ng 4 turn cu·ªëi)
                messages_for_model = [
                    {"role": "system", "content": system_prompt},
                    # g·ª≠i c√¢u h·ªèi g·ªëc ti·∫øng Vi·ªát d∆∞·ªõi d·∫°ng user message (ƒë·ªÉ model bi·∫øt y√™u c·∫ßu tr·∫£ l·ªùi ti·∫øng Vi·ªát)
                    {"role": "user", "content": f"Original question (VN): {prompt}"},
                    {"role": "user", "content": f"Search query used (EN): {query_en}"}
                ]
                # D√πng Groq streaming
                stream = groq_client.chat.completions.create(
                    messages=messages_for_model,
                    model=CONSTANTS["MODEL_NAME"],
                    stream=True,
                    temperature=0.2,
                    max_tokens=1024
                )

                # Hi·ªÉn th·ªã streaming: c·∫≠p nh·∫≠t message_placeholder d·∫ßn d·∫ßn
                for chunk in stream:
                    # chunk c√≥ c·∫•u tr√∫c t∆∞∆°ng t·ª± OpenAI streaming deltas
                    delta = chunk.choices[0].delta
                    if hasattr(delta, "content") and delta.content:
                        piece = delta.content
                        full_response += piece
                        # Hi·ªÉn th·ªã k√®m con tr·ªè
                        message_placeholder.markdown(full_response + "‚ñå")
                # Sau streaming k·∫øt th√∫c: th√™m ngu·ªìn n·∫øu c√≥
                if sources:
                    unique_sources = list(dict.fromkeys(sources))  # gi·ªØ th·ª© t·ª±, lo·∫°i tr√πng
                    sources_html = "<div class='source-box'>üìö <b>Ngu·ªìn tham kh·∫£o:</b><br>" + "<br>".join([f"‚Ä¢ {s}" for s in unique_sources]) + "</div>"
                    final = full_response + "\n\n" + sources_html
                    message_placeholder.markdown(final, unsafe_allow_html=True)
                    st.session_state.messages.append({"role": "assistant", "content": final})
                else:
                    message_placeholder.markdown(full_response)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})

            except Exception as e:
                # N·∫øu streaming kh√¥ng ƒë∆∞·ª£c, hi·ªÉn th·ªã l·ªói th√¢n thi·ªán
                err_msg = f"ƒê√£ x·∫£y ra l·ªói khi g·ªçi AI: {e}"
                message_placeholder.markdown(err_msg)
                st.session_state.messages.append({"role": "assistant", "content": err_msg})
