# ==============================================================================
#   D·ª∞ √ÅN KHKT: TR·ª¢ L√ù ·∫¢O TRA C·ª®U KI·∫æN TH·ª®C TIN H·ªåC (KTC AI)
#   T√°c gi·∫£: Nh√≥m KHKT THCS & THPT Ph·∫°m Ki·ªát
#   GVHD: Th·∫ßy Khanh
# ==============================================================================

import os
import glob
import time
import streamlit as st
from typing import List, Tuple

# --- AI & Data Processing Libraries ---
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# --- Translation Libraries ---
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# --- LLM Client ---
from groq import Groq

# ==============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG & H·∫∞NG S·ªê
# ==============================================================================

st.set_page_config(
    page_title="KTC Assistant - Tr·ª£ l√Ω Tin h·ªçc",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Settings
    LLM_MODEL = 'llama-3.1-8b-instant'
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    TRANSLATION_MODEL = "Helsinki-NLP/opus-mt-vi-en"
    
    # Data Settings
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    
    # RAG Settings
    CHUNK_SIZE = 800
    CHUNK_OVERLAP = 150
    TOP_K_RETRIEVAL = 3

# ==============================================================================
# 2. GIAO DI·ªÜN (CSS & STYLING)
# ==============================================================================

def inject_custom_css():
    st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Inter', sans-serif;
        }
        
        /* Header Styling */
        .main-header {
            background: linear-gradient(90deg, #0f4c81 0%, #00c6ff 100%);
            padding: 20px;
            border-radius: 10px;
            color: white;
            text-align: center;
            margin-bottom: 20px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .main-header h1 {
            margin: 0;
            font-size: 2.5rem;
            font-weight: 700;
            color: white !important;
        }
        .main-header p {
            font-size: 1.1rem;
            opacity: 0.9;
        }

        /* Chat Message Styling */
        .stChatMessage {
            border-radius: 10px;
            border: 1px solid #eee;
            padding: 10px;
            margin-bottom: 10px;
        }
        
        /* Source Expander */
        .streamlit-expanderHeader {
            font-weight: 600;
            color: #0f4c81;
        }
        
        /* Sidebar Info */
        .project-info {
            background-color: #f0f2f6;
            padding: 15px;
            border-radius: 8px;
            font-size: 0.9rem;
            border-left: 4px solid #0f4c81;
            margin-bottom: 20px;
        }
    </style>
    """, unsafe_allow_html=True)

# ==============================================================================
# 3. QU·∫¢N L√ù T√ÄI NGUY√äN (CACHING RESOURCE)
# ==============================================================================

@st.cache_resource(show_spinner=False)
def load_groq_client():
    """Kh·ªüi t·∫°o k·∫øt n·ªëi ƒë·∫øn Groq API."""
    try:
        api_key = st.secrets["GROQ_API_KEY"]
        return Groq(api_key=api_key)
    except Exception as e:
        st.error(f"‚ùå L·ªói c·∫•u h√¨nh API Key: {e}")
        return None

@st.cache_resource(show_spinner=False)
def load_embedding_model():
    """T·∫£i model Embedding (Ch·∫°y 1 l·∫ßn duy nh·∫•t)."""
    return HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)

@st.cache_resource(show_spinner=False)
def load_translator():
    """T·∫£i model D·ªãch thu·∫≠t (Ch·∫°y 1 l·∫ßn duy nh·∫•t)."""
    try:
        tokenizer = AutoTokenizer.from_pretrained(AppConfig.TRANSLATION_MODEL)
        model = AutoModelForSeq2SeqLM.from_pretrained(AppConfig.TRANSLATION_MODEL)
        translator = pipeline(
            "translation", 
            model=model, 
            tokenizer=tokenizer, 
            src_lang="vi", 
            tgt_lang="en"
        )
        return translator
    except Exception as e:
        print(f"Translator Error: {e}")
        return None

# ==============================================================================
# 4. X·ª¨ L√ù D·ªÆ LI·ªÜU & RAG LOGIC
# ==============================================================================

class KnowledgeBaseManager:
    def __init__(self):
        self.embeddings = load_embedding_model()
    
    def load_documents(self) -> List[Document]:
        """ƒê·ªçc to√†n b·ªô file PDF trong th∆∞ m·ª•c."""
        if not os.path.exists(AppConfig.PDF_DIR):
            os.makedirs(AppConfig.PDF_DIR)
            return []
        
        pdf_files = glob.glob(os.path.join(AppConfig.PDF_DIR, "*.pdf"))
        docs = []
        
        for pdf_path in pdf_files:
            try:
                reader = PdfReader(pdf_path)
                filename = os.path.basename(pdf_path)
                for i, page in enumerate(reader.pages):
                    text = page.extract_text()
                    if text and len(text.strip()) > 50: # Ch·ªâ l·∫•y trang c√≥ n·ªôi dung ƒë√°ng k·ªÉ
                        docs.append(Document(
                            page_content=text,
                            metadata={"source": filename, "page": i + 1}
                        ))
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file {pdf_path}. B·ªè qua.")
        
        return docs

    def get_vector_store(self, force_rebuild=False):
        """T·∫£i ho·∫∑c x√¢y d·ª±ng l·∫°i Vector Database."""
        if os.path.exists(AppConfig.VECTOR_DB_PATH) and not force_rebuild:
            try:
                return FAISS.load_local(
                    AppConfig.VECTOR_DB_PATH, 
                    self.embeddings, 
                    allow_dangerous_deserialization=True
                )
            except Exception:
                pass # N·∫øu l·ªói load th√¨ build l·∫°i t·ª´ ƒë·∫ßu

        # Build m·ªõi
        docs = self.load_documents()
        if not docs:
            return None
            
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=AppConfig.CHUNK_SIZE,
            chunk_overlap=AppConfig.CHUNK_OVERLAP
        )
        splits = splitter.split_documents(docs)
        
        vector_db = FAISS.from_documents(splits, self.embeddings)
        vector_db.save_local(AppConfig.VECTOR_DB_PATH)
        return vector_db

# ==============================================================================
# 5. C√ÅC H√ÄM H·ªñ TR·ª¢ (UTILITIES)
# ==============================================================================

def translate_query(text: str, translator) -> str:
    """D·ªãch c√¢u h·ªèi t·ª´ Vi·ªát sang Anh ƒë·ªÉ RAG ho·∫°t ƒë·ªông t·ªët h∆°n v·ªõi t√†i li·ªáu ti·∫øng Anh."""
    if not translator:
        return text
    try:
        # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh l·ªói model
        result = translator(text[:512]) 
        return result[0]['translation_text']
    except Exception:
        return text

def retrieve_info(vector_db, query: str) -> Tuple[str, List[str]]:
    """T√¨m ki·∫øm th√¥ng tin li√™n quan trong Vector DB."""
    if not vector_db:
        return "", []
    
    try:
        results = vector_db.similarity_search(query, k=AppConfig.TOP_K_RETRIEVAL)
        context_str = ""
        sources_list = []
        
        for doc in results:
            context_str += f"---\nN·ªôi dung: {doc.page_content}\n"
            sources_list.append(f"üìÑ {doc.metadata['source']} (Trang {doc.metadata['page']})")
            
        return context_str, list(set(sources_list)) # Remove duplicates
    except Exception as e:
        return "", []

def generate_response_stream(client, context, question):
    """T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ LLM (Streaming)."""
    system_prompt = f"""
    B·∫°n l√† KTC Assistant, m·ªôt tr·ª£ l√Ω gi√°o d·ª•c chuy√™n nghi·ªáp, th√¢n thi·ªán d√†nh cho h·ªçc sinh.
    Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n [TH√îNG TIN ƒê∆Ø·ª¢C CUNG C·∫§P] d∆∞·ªõi ƒë√¢y.
    
    Y√™u c·∫ßu:
    1. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, gi·ªçng vƒÉn s∆∞ ph·∫°m, d·ªÖ hi·ªÉu.
    2. N·∫øu th√¥ng tin c√≥ trong t√†i li·ªáu, h√£y gi·∫£i th√≠ch chi ti·∫øt.
    3. N·∫øu t√†i li·ªáu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i "Xin l·ªói, d·ªØ li·ªáu hi·ªán t·∫°i ch∆∞a c·∫≠p nh·∫≠t th√¥ng tin n√†y."
    4. Tr√¨nh b√†y ƒë·∫πp m·∫Øt (d√πng Markdown, bullet points).

    [TH√îNG TIN ƒê∆Ø·ª¢C CUNG C·∫§P]:
    {context}
    """
    
    try:
        stream = client.chat.completions.create(
            model=AppConfig.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            stream=True,
            temperature=0.3
        )
        return stream
    except Exception as e:
        return f"L·ªói k·∫øt n·ªëi AI: {str(e)}"

# ==============================================================================
# 6. MAIN APP LOOP 
# ==============================================================================

def main():
    inject_custom_css()
    
    # --- C·∫•u h√¨nh Sidebar (Thanh b√™n tr√°i) ---
    with st.sidebar:
        # Hi·ªÉn th·ªã Logo (D√πng c·ªôt ƒë·ªÉ cƒÉn gi·ªØa cho ƒë·∫πp)
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if os.path.exists("LOGO.jpg"):
                st.image("LOGO.jpg", use_container_width=True)
            else:
                st.title("ü§ñ")
        
        # Th√¥ng tin d·ª± √°n
        st.markdown("""
        <div class="project-info">
            <b>üèÜ D·ª∞ √ÅN KHKT 2024-2025</b><br>
            ƒê∆°n v·ªã: THCS & THPT Ph·∫°m Ki·ªát<br>
            T√°c gi·∫£: B√πi T√° T√πng & Cao S·ªπ B·∫£o Chung<br>
            GVHD: Th·∫ßy Khanh
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        st.subheader("‚öôÔ∏è ƒêi·ªÅu khi·ªÉn")
        
        # N√∫t c·∫≠p nh·∫≠t d·ªØ li·ªáu
        if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi", use_container_width=True, key="btn_update"):
            with st.spinner("ƒêang ƒë·ªçc t√†i li·ªáu v√† h·ªçc l·∫°i..."):
                kb = KnowledgeBaseManager()
                # Force rebuild v√† l∆∞u v√†o session_state
                st.session_state.vector_db = kb.get_vector_store(force_rebuild=True)
            st.success("ƒê√£ c·∫≠p nh·∫≠t ki·∫øn th·ª©c th√†nh c√¥ng!")
            time.sleep(1)
            st.rerun()

        # N√∫t x√≥a l·ªãch s·ª≠
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True, key="btn_clear"):
            st.session_state.messages = []
            st.rerun()

    # --- Giao di·ªán ch√≠nh (B√™n ph·∫£i) ---
    st.markdown("""
    <div class="main-header">
        <h1>üéì TR·ª¢ L√ù ·∫¢O KTC AI</h1>
        <p>H·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c Tin h·ªçc & Nghi√™n c·ª©u khoa h·ªçc</p>
    </div>
    """, unsafe_allow_html=True)

    # Kh·ªüi t·∫°o Session State cho tin nh·∫Øn
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ch√†o b·∫°n! M√¨nh l√† **KTC AI**. M√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b√†i h·ªçc h√¥m nay? üßë‚Äçüíª"}
        ]
    
    # Load Resources (Ch·ªâ load n·∫øu ch∆∞a c√≥ ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô)
    groq_client = load_groq_client()
    translator = load_translator()
    
    if "vector_db" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c..."):
            kb = KnowledgeBaseManager()
            st.session_state.vector_db = kb.get_vector_store()

    if not groq_client:
        st.error("‚ö†Ô∏è L·ªói API Key: Vui l√≤ng ki·ªÉm tra file c·∫•u h√¨nh secrets.")
        st.stop()

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # X·ª≠ l√Ω Chat Input
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y..."):
        # 1. Hi·ªÉn th·ªã c√¢u h·ªèi ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(prompt)

        # 2. X·ª≠ l√Ω logic AI
        with st.chat_message("assistant", avatar="ü§ñ"):
            message_placeholder = st.empty()
            
            # B∆∞·ªõc A: D·ªãch (n·∫øu c·∫ßn)
            search_query = prompt
            if translator:
                translated = translate_query(prompt, translator)
                if translated and translated != prompt:
                    search_query = translated

            # B∆∞·ªõc B: Truy v·∫•n RAG
            context_text, sources = retrieve_info(st.session_state.vector_db, search_query)
            
            # B∆∞·ªõc C: G·ªçi LLM (Ki·ªÉm tra context c√≥ r·ªóng kh√¥ng ƒë·ªÉ x·ª≠ l√Ω kh√©o h∆°n)
            if not context_text:
                context_text = "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu. H√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa b·∫°n."

            stream = generate_response_stream(groq_client, context_text, prompt)
            
            # B∆∞·ªõc D: Streaming ph·∫£n h·ªìi
            full_response = ""
            if isinstance(stream, str): # X·ª≠ l√Ω n·∫øu tr·∫£ v·ªÅ l·ªói chu·ªói
                full_response = stream
                message_placeholder.markdown(full_response)
            else:
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response + "‚ñå")
                message_placeholder.markdown(full_response)

            # B∆∞·ªõc E: Hi·ªÉn th·ªã ngu·ªìn tr√≠ch d·∫´n
            if sources:
                with st.expander("üìö T√†i li·ªáu tham kh·∫£o & Minh ch·ª©ng"):
                    for src in sources:
                        st.markdown(f"- {src}")
            
            # L∆∞u v√†o l·ªãch s·ª≠
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()