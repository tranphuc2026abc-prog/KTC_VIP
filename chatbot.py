import os
import glob
import time
import streamlit as st

# --- Imports t·ªëi ∆∞u & X·ª≠ l√Ω l·ªói th∆∞ vi·ªán ---
try:
    from pypdf import PdfReader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
    from groq import Groq
except ImportError as e:
    st.error(f"‚ùå L·ªói th∆∞ vi·ªán: {e}. Vui l√≤ng ki·ªÉm tra file requirements.txt")
    st.stop()

# ==============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG)
# ==============================================================================

st.set_page_config(
    page_title="KTC Assistant - Tr·ª£ l√Ω KHKT",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    """C·∫•u h√¨nh trung t√¢m cho ·ª©ng d·ª•ng."""
    # Model AI
    LLM_MODEL = 'llama-3.1-8b-instant'
    # Embedding nh·∫π, t·ªëi ∆∞u cho CPU
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    TRANSLATION_MODEL = "Helsinki-NLP/opus-mt-vi-en"
    
    # ƒê∆∞·ªùng d·∫´n
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    LOGO_PATH = "LOGO.jpg"
    
    # Tham s·ªë RAG
    CHUNK_SIZE = 1000 
    CHUNK_OVERLAP = 200
    TOP_K_RETRIEVAL = 4 # Gi·∫£m xu·ªëng 4 ƒë·ªÉ l·∫•y ng·ªØ c·∫£nh ch·∫Øt l·ªçc nh·∫•t

# ==============================================================================
# 2. UI/UX: GIAO DI·ªÜN & CSS
# ==============================================================================

def inject_custom_css():
    st.markdown("""
    <style>
        /* Import Font ƒë·∫πp */
        @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Roboto', sans-serif;
        }

        /* Header Gradient */
        .main-header {
            background: linear-gradient(90deg, #005C97 0%, #363795 100%);
            padding: 1.5rem;
            border-radius: 15px;
            color: white;
            text-align: center;
            margin-bottom: 2rem;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .main-header h1 {
            color: white !important;
            font-weight: 700;
            margin: 0;
            font-size: 2rem;
        }
        .main-header p {
            margin-top: 0.5rem;
            opacity: 0.9;
            font-size: 1.1rem;
        }

        /* Sidebar Styling */
        [data-testid="stSidebar"] {
            background-color: #f8f9fa;
        }
        .sidebar-card {
            background: white;
            padding: 15px;
            border-radius: 10px;
            border-left: 5px solid #363795;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            margin-bottom: 20px;
        }
        .sidebar-card h4 {
            color: #363795;
            margin-top: 0;
            font-size: 1rem;
            font-weight: bold;
        }
        
        /* Chat Message Styling */
        .stChatMessage {
            border-radius: 10px;
            border: 1px solid #f0f2f6;
        }
        /* User Avatar Wrapper */
        .stChatMessage[data-testid="stChatMessage"]:nth-child(odd) {
             background-color: #f0f7ff;
        }
    </style>
    """, unsafe_allow_html=True)

# ==============================================================================
# 3. QU·∫¢N L√ù T√ÄI NGUY√äN (CACHING & RESOURCE)
# ==============================================================================

@st.cache_resource(show_spinner=False)
def load_groq_client():
    """Kh·ªüi t·∫°o Groq Client (Cache tr·ªçn ƒë·ªùi phi√™n ch·∫°y)."""
    api_key = st.secrets.get("GROQ_API_KEY")
    if not api_key:
        return None
    return Groq(api_key=api_key)

@st.cache_resource(show_spinner=False)
def load_embedding_model():
    """Load model Vector h√≥a (N·∫∑ng -> Cache)."""
    try:
        return HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)
    except Exception:
        return None

@st.cache_resource(show_spinner=False)
def load_translator():
    """Load model D·ªãch thu·∫≠t (R·∫•t n·∫∑ng -> Cache k·ªπ)."""
    try:
        tokenizer = AutoTokenizer.from_pretrained(AppConfig.TRANSLATION_MODEL)
        model = AutoModelForSeq2SeqLM.from_pretrained(AppConfig.TRANSLATION_MODEL)
        return pipeline("translation", model=model, tokenizer=tokenizer, src_lang="vi", tgt_lang="en")
    except Exception:
        return None

@st.cache_data(show_spinner=False)
def load_and_process_pdfs(pdf_dir):
    """ƒê·ªçc PDF v√† chia nh·ªè vƒÉn b·∫£n (Cache data ƒë·∫ßu ra)."""
    docs = []
    if not os.path.exists(pdf_dir):
        return docs
    
    pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
    for pdf_path in pdf_files:
        try:
            reader = PdfReader(pdf_path)
            filename = os.path.basename(pdf_path)
            for i, page in enumerate(reader.pages):
                text = page.extract_text()
                if text and len(text.strip()) > 50:
                    docs.append(Document(
                        page_content=text,
                        metadata={"source": filename, "page": i + 1}
                    ))
        except Exception:
            continue
            
    # Split text
    if docs:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=AppConfig.CHUNK_SIZE,
            chunk_overlap=AppConfig.CHUNK_OVERLAP
        )
        return splitter.split_documents(docs)
    return []

# ==============================================================================
# 4. CORE LOGIC: RAG & AI PROCESSING
# ==============================================================================

class KnowledgeBase:
    def __init__(self):
        self.embeddings = load_embedding_model()

    def get_vector_store(self):
        """L·∫•y Vector Store: ∆Øu ti√™n load t·ª´ ·ªï c·ª©ng, n·∫øu kh√¥ng c√≥ th√¨ t·∫°o m·ªõi."""
        if not self.embeddings:
            return None

        # 1. Th·ª≠ load t·ª´ Disk
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                return FAISS.load_local(
                    AppConfig.VECTOR_DB_PATH, 
                    self.embeddings, 
                    allow_dangerous_deserialization=True
                )
            except Exception:
                st.toast("‚ö†Ô∏è Database c≈© l·ªói, ƒëang t·∫°o m·ªõi...", icon="üîÑ")
        
        # 2. T·∫°o m·ªõi n·∫øu c·∫ßn
        return self._create_new_db()

    def _create_new_db(self):
        splits = load_and_process_pdfs(AppConfig.PDF_DIR)
        if not splits:
            return None
        
        try:
            vector_db = FAISS.from_documents(splits, self.embeddings)
            vector_db.save_local(AppConfig.VECTOR_DB_PATH)
            return vector_db
        except Exception as e:
            st.error(f"L·ªói t·∫°o Vector DB: {e}")
            return None

def translate_query(text, translator):
    """D·ªãch c√¢u h·ªèi sang ti·∫øng Anh (n·∫øu model ƒë√£ load)."""
    if not translator: 
        return text
    try:
        # Gi·ªõi h·∫°n k√Ω t·ª± ƒë·ªÉ tr√°nh l·ªói model
        result = translator(text[:500])
        return result[0]['translation_text'] if result else text
    except Exception:
        return text

def get_context(vector_db, query):
    """T√¨m ki·∫øm th√¥ng tin li√™n quan."""
    if not vector_db:
        return "", []
    try:
        # Similarity search
        results = vector_db.similarity_search(query, k=AppConfig.TOP_K_RETRIEVAL)
        
        context_text = ""
        sources = []
        
        for doc in results:
            src = doc.metadata.get('source', 'T√†i li·ªáu')
            page = doc.metadata.get('page', '1')
            content = doc.page_content.replace("\n", " ")
            
            context_text += f"\n[Ngu·ªìn: {src} - Tr.{page}]: {content}"
            sources.append(f"{src} (Trang {page})")
            
        return context_text, list(set(sources)) # Unique sources
    except Exception:
        return "", []

def generate_stream(client, context, question):
    """T·∫°o response stream t·ª´ Groq."""
    system_prompt = f"""
    B·∫°n l√† KTC Assistant - Tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ h·ªçc t·∫≠p v√† nghi√™n c·ª©u khoa h·ªçc.
    
    NHI·ªÜM V·ª§:
    - Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong [CONTEXT].
    - N·∫øu [CONTEXT] kh√¥ng c√≥ th√¥ng tin, h√£y d√πng ki·∫øn th·ª©c Tin h·ªçc chu·∫©n c·ªßa b·∫°n (CT GDPT 2018).
    - VƒÉn phong: Th√¢n thi·ªán, s∆∞ ph·∫°m, khuy·∫øn kh√≠ch h·ªçc sinh.
    - ƒê·ªãnh d·∫°ng: S·ª≠ d·ª•ng Markdown (in ƒë·∫≠m, danh s√°ch) ƒë·ªÉ d·ªÖ ƒë·ªçc.

    [CONTEXT D·ªÆ LI·ªÜU]:
    {context}
    """
    
    try:
        completion = client.chat.completions.create(
            model=AppConfig.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            stream=True,
            temperature=0.3
        )
        
        for chunk in completion:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                
    except Exception as e:
        yield f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi AI: {str(e)}"

# ==============================================================================
# 5. MAIN APPLICATION
# ==============================================================================

def main():
    inject_custom_css()
    
    # --- Sidebar ---
    with st.sidebar:
        if os.path.exists(AppConfig.LOGO_PATH):
            st.image(AppConfig.LOGO_PATH, use_container_width=True)
        else:
            st.header("ü§ñ KTC AI")

        st.markdown("---")
        
        # Th√¥ng tin d·ª± √°n (Update theo y√™u c·∫ßu c·ªßa Th·∫ßy)
        st.markdown("""
        <div class="sidebar-card">
            <h4>üèÜ S·∫¢N PH·∫®M D·ª∞ THI KHKT<br>C·∫§P TR∆Ø·ªúNG</h4>
            <p style="font-size: 0.9rem; margin-bottom: 5px;"><b>üè´ ƒê∆°n v·ªã:</b> THCS & THPT Ph·∫°m Ki·ªát</p>
            <p style="font-size: 0.9rem; margin-bottom: 5px;"><b>üë®‚Äçüíª T√°c gi·∫£:</b><br>- B√πi T√° T√πng<br>- Cao S·ªπ B·∫£o Chung</p>
            <p style="font-size: 0.9rem;"><b>üßë‚Äçüè´ GVHD:</b> Th·∫ßy Khanh</p>
        </div>
        """, unsafe_allow_html=True)
        
        with st.expander("üõ†Ô∏è C√†i ƒë·∫∑t n√¢ng cao"):
            top_k = st.slider("ƒê·ªô s√¢u t√¨m ki·∫øm (Chunks)", 1, 10, AppConfig.TOP_K_RETRIEVAL)
            AppConfig.TOP_K_RETRIEVAL = top_k
            st.info("TƒÉng ƒë·ªô s√¢u gi√∫p t√¨m nhi·ªÅu th√¥ng tin h∆°n nh∆∞ng c√≥ th·ªÉ l√†m c√¢u tr·∫£ l·ªùi b·ªã lo√£ng.")

        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

    # --- Header ---
    st.markdown("""
    <div class="main-header">
        <h1>üéì TR·ª¢ L√ù ·∫¢O KTC AI</h1>
        <p>H·ªá th·ªëng h·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c Tin h·ªçc & Nghi√™n c·ª©u khoa h·ªçc</p>
    </div>
    """, unsafe_allow_html=True)

    # --- Init State ---
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ch√†o b·∫°n! M√¨nh l√† tr·ª£ l√Ω ·∫£o KTC. M√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho d·ª± √°n KHKT c·ªßa b·∫°n h√¥m nay?"}
        ]

    # --- Load Resources ---
    groq_client = load_groq_client()
    translator = load_translator()
    
    # Load DB (Silent)
    if "vector_db" not in st.session_state:
        kb = KnowledgeBase()
        st.session_state.vector_db = kb.get_vector_store()

    # Check API Key
    if not groq_client:
        st.warning("‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh GROQ_API_KEY. Vui l√≤ng ki·ªÉm tra secrets.toml")
        st.stop()

    # --- Chat Interface ---
    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # --- Input Processing ---
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y..."):
        # Hi·ªÉn th·ªã c√¢u h·ªèi User
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(prompt)

        # X·ª≠ l√Ω AI
        with st.chat_message("assistant", avatar="ü§ñ"):
            response_container = st.empty()
            
            # Quy tr√¨nh x·ª≠ l√Ω (Hi·ªÉn th·ªã tr·∫°ng th√°i ƒë·∫πp)
            with st.status("üöÄ ƒêang x·ª≠ l√Ω...", expanded=True) as status:
                
                # 1. D·ªãch thu·∫≠t
                search_query = prompt
                if translator:
                    st.write("üåç ƒêang t·ªëi ∆∞u h√≥a c√¢u h·ªèi (D·ªãch Vi·ªát -> Anh)...")
                    search_query = translate_query(prompt, translator)
                
                # 2. RAG Retrieval
                st.write("üìö ƒêang tra c·ª©u t√†i li·ªáu chuy√™n ng√†nh...")
                context, sources = get_context(st.session_state.vector_db, search_query)
                
                if context:
                    st.write(f"‚úÖ T√¨m th·∫•y {len(sources)} ngu·ªìn t√†i li·ªáu li√™n quan.")
                else:
                    st.write("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y trong t√†i li·ªáu, s·ª≠ d·ª•ng ki·∫øn th·ª©c n·ªÅn.")
                
                status.update(label="‚úÖ ƒê√£ xong!", state="complete", expanded=False)

            # 3. Stream Response
            full_response = ""
            stream = generate_stream(groq_client, context, prompt)
            
            for chunk in stream:
                full_response += chunk
                response_container.markdown(full_response + "‚ñå")
            
            response_container.markdown(full_response)
            
            # Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o (N·∫øu c√≥)
            if sources:
                with st.expander("üìñ Xem ngu·ªìn t√†i li·ªáu tham kh·∫£o"):
                    for src in sources:
                        st.caption(f"‚Ä¢ {src}")

            # L∆∞u l·ªãch s·ª≠
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()