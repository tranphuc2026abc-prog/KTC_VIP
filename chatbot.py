import os
import glob
import time
import streamlit as st
from typing import List, Tuple, Optional

# --- AI & Data Processing Libraries ---
# T·ªëi ∆∞u import ƒë·ªÉ tr√°nh n·∫°p th∆∞ vi·ªán kh√¥ng c·∫ßn thi·∫øt n·∫øu ch∆∞a d√πng
try:
    from pypdf import PdfReader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
    from groq import Groq
except ImportError as e:
    st.error(f"‚ùå Thi·∫øu th∆∞ vi·ªán: {e}. Vui l√≤ng ch·∫°y: pip install -r requirements.txt")
    st.stop()

# ==============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG)
# ==============================================================================

st.set_page_config(
    page_title="KTC Assistant - Tr·ª£ l√Ω Tin h·ªçc",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    """Class ch·ª©a to√†n b·ªô c·∫•u h√¨nh ƒë·ªÉ d·ªÖ d√†ng qu·∫£n l√Ω v√† thay ƒë·ªïi."""
    # Model Settings
    LLM_MODEL = 'llama-3.1-8b-instant'
    # Model Embedding nh·∫π nh∆∞ng hi·ªáu qu·∫£ cho ti·∫øng Vi·ªát/Anh
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    TRANSLATION_MODEL = "Helsinki-NLP/opus-mt-vi-en"
    
    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    LOGO_PATH = "LOGO.jpg"
    
    # RAG Parameters
    CHUNK_SIZE = 1000 
    CHUNK_OVERLAP = 200
    TOP_K_RETRIEVAL = 5 # Gi·ªØ ·ªü m·ª©c 5 ƒë·ªÉ c√¢n b·∫±ng t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c

# ==============================================================================
# 2. GIAO DI·ªÜN & CSS (UI/UX)
# ==============================================================================

def inject_custom_css():
    """CSS t√πy ch·ªânh ƒë·ªÉ giao di·ªán s·∫°ch, ƒë·∫πp v√† chuy√™n nghi·ªáp h∆°n."""
    st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Inter', sans-serif;
        }
        
        /* Tinh ch·ªânh Header ch√≠nh */
        .main-header {
            background: linear-gradient(135deg, #0f4c81 0%, #00c6ff 100%);
            padding: 20px;
            border-radius: 12px;
            color: white;
            text-align: center;
            margin-bottom: 25px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        .main-header h1 {
            font-size: 2.2rem;
            font-weight: 800;
            margin: 0;
            color: #ffffff !important;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }
        .main-header p {
            font-size: 1.1rem;
            opacity: 0.95;
            margin-top: 5px;
        }

        /* Tinh ch·ªânh Sidebar */
        [data-testid="stSidebar"] {
            background-color: #f8f9fa;
        }
        .sidebar-info {
            background-color: #ffffff;
            padding: 15px;
            border-radius: 10px;
            border: 1px solid #e0e0e0;
            border-left: 5px solid #0f4c81;
            margin-bottom: 15px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        }
        .sidebar-title {
            color: #0f4c81;
            font-weight: 800;
            text-align: center;
            font-size: 0.9rem;
            margin-bottom: 10px;
            text-transform: uppercase;
        }
        .sidebar-text {
            font-size: 0.85rem;
            color: #333;
            line-height: 1.5;
        }
        
        /* Bong b√≥ng chat */
        .stChatMessage {
            border-radius: 10px;
            padding: 10px;
        }
    </style>
    """, unsafe_allow_html=True)

# ==============================================================================
# 3. QU·∫¢N L√ù T√ÄI NGUY√äN (CACHING & INITIALIZATION)
# ==============================================================================

@st.cache_resource(show_spinner=False)
def load_groq_client():
    """Kh·ªüi t·∫°o Groq Client an to√†n."""
    try:
        api_key = st.secrets.get("GROQ_API_KEY")
        if not api_key:
            st.error("‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh GROQ_API_KEY trong Secrets.")
            return None
        return Groq(api_key=api_key)
    except Exception as e:
        st.error(f"‚ùå L·ªói k·∫øt n·ªëi Groq: {e}")
        return None

@st.cache_resource(show_spinner=False)
def load_embedding_model():
    """Load model vector h√≥a (ch·∫°y 1 l·∫ßn)."""
    try:
        return HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)
    except Exception as e:
        st.error(f"‚ùå L·ªói t·∫£i Embedding Model: {e}")
        return None

@st.cache_resource(show_spinner=False)
def load_translator():
    """Load model d·ªãch thu·∫≠t (ch·∫°y 1 l·∫ßn)."""
    try:
        # S·ª≠ d·ª•ng device=-1 cho CPU (Streamlit Cloud th∆∞·ªùng kh√¥ng c√≥ GPU)
        tokenizer = AutoTokenizer.from_pretrained(AppConfig.TRANSLATION_MODEL)
        model = AutoModelForSeq2SeqLM.from_pretrained(AppConfig.TRANSLATION_MODEL)
        translator = pipeline("translation", model=model, tokenizer=tokenizer, src_lang="vi", tgt_lang="en")
        return translator
    except Exception as e:
        # Kh√¥ng return None ƒë·ªÉ app v·∫´n ch·∫°y ƒë∆∞·ª£c d√π kh√¥ng c√≥ d·ªãch
        print(f"Translator Warning: {e}") 
        return None

# ==============================================================================
# 4. LOGIC X·ª¨ L√ù D·ªÆ LI·ªÜU & RAG (CORE)
# ==============================================================================

class KnowledgeBaseManager:
    """Qu·∫£n l√Ω vi·ªác ƒë·ªçc PDF v√† t·∫°o Vector DB."""
    
    def __init__(self):
        self.embeddings = load_embedding_model()
    
    def get_vector_store(self):
        """L·∫•y Vector Store, n·∫øu ch∆∞a c√≥ th√¨ t·ª± build."""
        if not self.embeddings:
            return None
            
        # 1. Th·ª≠ load t·ª´ ·ªï c·ª©ng
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                return FAISS.load_local(
                    AppConfig.VECTOR_DB_PATH, 
                    self.embeddings, 
                    allow_dangerous_deserialization=True
                )
            except Exception:
                st.toast("‚ö†Ô∏è Database l·ªói, ƒëang t·∫°o l·∫°i...", icon="üîÑ")
        
        # 2. N·∫øu ch∆∞a c√≥ ho·∫∑c l·ªói, build m·ªõi
        return self._build_new_vector_store()

    def _build_new_vector_store(self):
        """H√†m n·ªôi b·ªô ƒë·ªÉ ƒë·ªçc PDF v√† t·∫°o index."""
        if not os.path.exists(AppConfig.PDF_DIR):
            os.makedirs(AppConfig.PDF_DIR)
            return None
            
        pdf_files = glob.glob(os.path.join(AppConfig.PDF_DIR, "*.pdf"))
        if not pdf_files:
            return None
            
        docs = []
        status_text = st.empty()
        status_text.info(f"üìö ƒêang n·∫°p {len(pdf_files)} t√†i li·ªáu PDF...")
        
        for pdf_path in pdf_files:
            try:
                reader = PdfReader(pdf_path)
                filename = os.path.basename(pdf_path)
                for i, page in enumerate(reader.pages):
                    text = page.extract_text()
                    if text and len(text.strip()) > 50:
                        docs.append(Document(
                            page_content=text,
                            metadata={"source": filename, "page": i + 1}
                        ))
            except Exception:
                continue # B·ªè qua file l·ªói
        
        status_text.empty() # X√≥a th√¥ng b√°o
        
        if not docs:
            return None

        # Chia nh·ªè vƒÉn b·∫£n
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=AppConfig.CHUNK_SIZE,
            chunk_overlap=AppConfig.CHUNK_OVERLAP
        )
        splits = splitter.split_documents(docs)
        
        # T·∫°o v√† l∆∞u DB
        vector_db = FAISS.from_documents(splits, self.embeddings)
        vector_db.save_local(AppConfig.VECTOR_DB_PATH)
        return vector_db

# ==============================================================================
# 5. UTILITIES (H√ÄM H·ªñ TR·ª¢)
# ==============================================================================

def translate_query(text: str, translator) -> str:
    """D·ªãch c√¢u h·ªèi sang ti·∫øng Anh."""
    if not translator: return text
    try:
        return translator(text[:512])[0]['translation_text']
    except Exception:
        return text

def retrieve_info(vector_db, query: str) -> Tuple[str, List[str]]:
    """T√¨m ki·∫øm th√¥ng tin trong Vector DB."""
    if not vector_db:
        return "", []
    try:
        # T√¨m ki·∫øm similarity
        results = vector_db.similarity_search(query, k=AppConfig.TOP_K_RETRIEVAL)
        context = "\n\n".join([f"[Ngu·ªìn: {d.metadata['source']} - Tr. {d.metadata['page']}]\n{d.page_content}" for d in results])
        sources = list(set([f"{d.metadata['source']} (Trang {d.metadata['page']})" for d in results]))
        return context, sources
    except Exception:
        return "", []

def generate_stream_response(client, context, question):
    """G·ªçi LLM tr·∫£ v·ªÅ Stream."""
    system_prompt = f"""
    B·∫°n l√† KTC Assistant, m·ªôt tr·ª£ l√Ω gi√°o d·ª•c ·∫£o, chuy√™n gia v·ªÅ Tin h·ªçc.
    
    NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
    
    NGUY√äN T·∫ÆC:
    1. ∆Øu ti√™n d√πng th√¥ng tin trong [CONTEXT]. N·∫øu kh√¥ng c√≥, h√£y d√πng ki·∫øn th·ª©c chu·∫©n c·ªßa b·∫°n v·ªÅ Tin h·ªçc (GDPT 2018).
    2. Tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát, vƒÉn phong s∆∞ ph·∫°m, d·ªÖ hi·ªÉu, th√¢n thi·ªán.
    3. D√πng Markdown ƒë·ªÉ tr√¨nh b√†y (in ƒë·∫≠m t·ª´ kh√≥a, g·∫°ch ƒë·∫ßu d√≤ng).
    
    [CONTEXT - D·ªÆ LI·ªÜU TRA C·ª®U]:
    {context}
    """
    
    try:
        return client.chat.completions.create(
            model=AppConfig.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            stream=True,
            temperature=0.3 # Gi·∫£m nhi·ªát ƒë·ªô ƒë·ªÉ c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n v·ªõi t√†i li·ªáu
        )
    except Exception as e:
        return f"‚ùå L·ªói k·∫øt n·ªëi AI: {str(e)}"

# ==============================================================================
# 6. MAIN APP (ƒê√£ s·ª≠a l·ªói st.toast icon)
# ==============================================================================

def main():
    inject_custom_css()
    
    # --- Sidebar ---
    with st.sidebar:
        # Logo cƒÉn gi·ªØa ƒë·∫πp m·∫Øt
        if os.path.exists(AppConfig.LOGO_PATH):
            col1, col2, col3 = st.columns([1, 4, 1])
            with col2:
                st.image(AppConfig.LOGO_PATH, use_container_width=True)
        else:
            st.markdown("<div style='text-align:center; font-size: 50px;'>ü§ñ</div>", unsafe_allow_html=True)

        st.markdown("---")
        
        # Th√¥ng tin d·ª± √°n clean v√† chuy√™n nghi·ªáp
        st.markdown("""
        <div class="sidebar-info">
            <div class="sidebar-title">üèÜ S·∫¢N PH·∫®M D·ª∞ THI<br>KHKT C·∫§P TR∆Ø·ªúNG</div>
            <div class="sidebar-text">
                <b>ƒê∆°n v·ªã:</b> THCS & THPT Ph·∫°m Ki·ªát<br>
                <b>T√°c gi·∫£:</b> B√πi T√° T√πng & Cao S·ªπ B·∫£o Chung<br>
                <b>GVHD:</b> Th·∫ßy Khanh
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # N√∫t ch·ª©c nƒÉng
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

    # --- Main Interface ---
    st.markdown("""
    <div class="main-header">
        <h1>üéì TR·ª¢ L√ù ·∫¢O KTC AI</h1>
        <p>H·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c Tin h·ªçc & Nghi√™n c·ª©u khoa h·ªçc</p>
    </div>
    """, unsafe_allow_html=True)

    # Kh·ªüi t·∫°o Session State
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ch√†o b·∫°n! Th·∫ßy Khanh v√† nh√≥m KHKT ƒë√£ n·∫°p ƒë·∫ßy ƒë·ªß d·ªØ li·ªáu. B·∫°n c·∫ßn t√¨m hi·ªÉu ki·∫øn th·ª©c g√¨ n√†o? üßë‚Äçüíª"}
        ]

    # Load Resources (Ch·ªâ load 1 l·∫ßn)
    groq_client = load_groq_client()
    translator = load_translator()
    
    # Check Vector DB (Lazy loading ƒë·ªÉ app m·ªü nhanh h∆°n)
    if "vector_db" not in st.session_state:
        kb = KnowledgeBaseManager()
        db = kb.get_vector_store()
        if db:
            st.session_state.vector_db = db
            # --- ƒê√É S·ª¨A D√íNG N√ÄY ---
            st.toast("‚úÖ ƒê√£ n·∫°p d·ªØ li·ªáu th√†nh c√¥ng!", icon="‚úÖ") 
        else:
            st.session_state.vector_db = None
            # Kh√¥ng b√°o l·ªói ngay, ƒë·ªÉ ng∆∞·ªùi d√πng v·∫´n chat ƒë∆∞·ª£c (nh∆∞ng AI s·∫Ω tr·∫£ l·ªùi chay)

    if not groq_client:
        st.warning("‚ö†Ô∏è H·ªá th·ªëng ƒëang b·∫£o tr√¨ k·∫øt n·ªëi AI. Vui l√≤ng ki·ªÉm tra l·∫°i sau.")
        st.stop()

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # X·ª≠ l√Ω input ng∆∞·ªùi d√πng
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (V√≠ d·ª•: C·∫•u tr√∫c r·∫Ω nh√°nh l√† g√¨?)..."):
        # 1. Hi·ªÉn th·ªã c√¢u h·ªèi ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(prompt)

        # 2. AI x·ª≠ l√Ω (D√πng st.status ƒë·ªÉ hi·ªÉn th·ªã quy tr√¨nh - R·∫•t t·ªët cho thi KHKT)
        with st.chat_message("assistant", avatar="ü§ñ"):
            response_placeholder = st.empty()
            full_response = ""
            sources = []
            
            with st.status("üîç H·ªá th·ªëng ƒëang ph√¢n t√≠ch...", expanded=True) as status:
                
                # B∆∞·ªõc 1: D·ªãch thu·∫≠t (N·∫øu c·∫ßn)
                search_query = prompt
                if translator:
                    st.write("üá¨üáß ƒêang d·ªãch c√¢u h·ªèi sang ti·∫øng Anh ƒë·ªÉ tra c·ª©u s√¢u h∆°n...")
                    translated = translate_query(prompt, translator)
                    if translated != prompt:
                        search_query = translated

                # B∆∞·ªõc 2: Truy xu·∫•t d·ªØ li·ªáu (RAG)
                st.write("üìö ƒêang qu√©t c∆° s·ªü d·ªØ li·ªáu PDF...")
                context_text, sources = retrieve_info(st.session_state.get("vector_db"), search_query)
                
                if not context_text:
                    context_text = "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu trong s√°ch. S·ª≠ d·ª•ng ki·∫øn th·ª©c n·ªÅn t·∫£ng."
                    st.write("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y trong t√†i li·ªáu, s·ª≠ d·ª•ng ki·∫øn th·ª©c AI.")
                else:
                    st.write("‚úÖ ƒê√£ t√¨m th·∫•y th√¥ng tin li√™n quan.")
                
                status.update(label="‚úÖ ƒê√£ x·ª≠ l√Ω xong!", state="complete", expanded=False)

            # B∆∞·ªõc 3: Streaming c√¢u tr·∫£ l·ªùi
            stream = generate_stream_response(groq_client, context_text, prompt)
            
            if isinstance(stream, str): # Tr∆∞·ªùng h·ª£p l·ªói tr·∫£ v·ªÅ string
                full_response = stream
                response_placeholder.markdown(full_response)
            else:
                for chunk in stream:
                    content = chunk.choices[0].delta.content
                    if content:
                        full_response += content
                        response_placeholder.markdown(full_response + "‚ñå")
                response_placeholder.markdown(full_response)

            # B∆∞·ªõc 4: Hi·ªÉn th·ªã ngu·ªìn (Minh ch·ª©ng khoa h·ªçc)
            if sources:
                with st.expander("üìñ Ngu·ªìn t√†i li·ªáu tham kh·∫£o"):
                    for src in sources:
                        st.markdown(f"- {src}")

            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()