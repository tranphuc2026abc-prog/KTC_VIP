# ==============================================================================
#   D·ª∞ √ÅN KHKT: TR·ª¢ L√ù ·∫¢O TRA C·ª®U KI·∫æN TH·ª®C TIN H·ªåC (KTC AI)
#   T√°c gi·∫£: Nh√≥m KHKT THCS & THPT Ph·∫°m Ki·ªát
#   GVHD: Th·∫ßy Khanh
# ==============================================================================

import os
import glob
import time
import streamlit as st
from typing import List, Tuple

# --- AI & Data Processing Libraries ---
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# --- Translation Libraries ---
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# --- LLM Client ---
from groq import Groq

# ==============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG & H·∫∞NG S·ªê
# ==============================================================================

st.set_page_config(
    page_title="KTC Assistant - Tr·ª£ l√Ω Tin h·ªçc",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Settings
    LLM_MODEL = 'llama-3.1-8b-instant'
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    TRANSLATION_MODEL = "Helsinki-NLP/opus-mt-vi-en"
    
    # Data Settings
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    
    # RAG Settings (ƒê√£ tinh ch·ªânh ƒë·ªÉ t√¨m ki·∫øm s√¢u h∆°n)
    CHUNK_SIZE = 1000 # TƒÉng k√≠ch th∆∞·ªõc ƒëo·∫°n ƒë·ªçc ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh t·ªët h∆°n
    CHUNK_OVERLAP = 200
    TOP_K_RETRIEVAL = 6 # TƒÉng s·ªë l∆∞·ª£ng ngu·ªìn tham kh·∫£o l√™n 6 ƒë·ªÉ tr√°nh b·ªè s√≥t

# ==============================================================================
# 2. GIAO DI·ªÜN (CSS & STYLING) - ƒê√É T·ªêI ∆ØU CHO G·ªåN
# ==============================================================================

def inject_custom_css():
    st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Inter', sans-serif;
        }
        
        /* Thu g·ªçn kho·∫£ng c√°ch ƒë·∫ßu trang ƒë·ªÉ ƒë·ª° t·ªën di·ªán t√≠ch */
        .block-container {
            padding-top: 2rem !important; 
            padding-bottom: 2rem !important;
        }
        
        /* Header Styling */
        .main-header {
            background: linear-gradient(90deg, #0f4c81 0%, #00c6ff 100%);
            padding: 15px; /* Gi·∫£m padding */
            border-radius: 10px;
            color: white;
            text-align: center;
            margin-bottom: 15px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .main-header h1 {
            margin: 0;
            font-size: 2rem; /* Gi·∫£m c·ª° ch·ªØ ti√™u ƒë·ªÅ ch√∫t */
            font-weight: 700;
            color: white !important;
        }
        .main-header p {
            font-size: 1rem;
            opacity: 0.9;
            margin-bottom: 0px;
        }

        /* Sidebar Info - L√†m g·ªçn t·ªëi ƒëa */
        .project-info {
            background-color: #f0f2f6;
            padding: 10px; /* Gi·∫£m padding */
            border-radius: 8px;
            font-size: 0.85rem; /* Ch·ªØ nh·ªè l·∫°i x√≠u cho g·ªçn */
            line-height: 1.4;
            border-left: 4px solid #0f4c81;
            margin-bottom: 10px;
        }
        
        /* ·∫®n b·ªõt kho·∫£ng tr·∫Øng th·ª´a m·∫∑c ƒë·ªãnh c·ªßa Streamlit Sidebar */
        section[data-testid="stSidebar"] > div {
            padding-top: 1rem;
        }
    </style>
    """, unsafe_allow_html=True)

# ==============================================================================
# 3. QU·∫¢N L√ù T√ÄI NGUY√äN (CACHING RESOURCE)
# ==============================================================================

@st.cache_resource(show_spinner=False)
def load_groq_client():
    """Kh·ªüi t·∫°o k·∫øt n·ªëi ƒë·∫øn Groq API."""
    try:
        api_key = st.secrets["GROQ_API_KEY"]
        return Groq(api_key=api_key)
    except Exception as e:
        st.error(f"‚ùå L·ªói c·∫•u h√¨nh API Key: {e}")
        return None

@st.cache_resource(show_spinner=False)
def load_embedding_model():
    """T·∫£i model Embedding (Ch·∫°y 1 l·∫ßn duy nh·∫•t)."""
    return HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)

@st.cache_resource(show_spinner=False)
def load_translator():
    """T·∫£i model D·ªãch thu·∫≠t (Ch·∫°y 1 l·∫ßn duy nh·∫•t)."""
    try:
        tokenizer = AutoTokenizer.from_pretrained(AppConfig.TRANSLATION_MODEL)
        model = AutoModelForSeq2SeqLM.from_pretrained(AppConfig.TRANSLATION_MODEL)
        translator = pipeline(
            "translation", 
            model=model, 
            tokenizer=tokenizer, 
            src_lang="vi", 
            tgt_lang="en"
        )
        return translator
    except Exception as e:
        print(f"Translator Error: {e}")
        return None

# ==============================================================================
# 4. X·ª¨ L√ù D·ªÆ LI·ªÜU & RAG LOGIC
# ==============================================================================

class KnowledgeBaseManager:
    def __init__(self):
        self.embeddings = load_embedding_model()
    
    def load_documents(self) -> List[Document]:
        """ƒê·ªçc to√†n b·ªô file PDF trong th∆∞ m·ª•c."""
        if not os.path.exists(AppConfig.PDF_DIR):
            os.makedirs(AppConfig.PDF_DIR)
            return []
        
        pdf_files = glob.glob(os.path.join(AppConfig.PDF_DIR, "*.pdf"))
        docs = []
        
        for pdf_path in pdf_files:
            try:
                reader = PdfReader(pdf_path)
                filename = os.path.basename(pdf_path)
                for i, page in enumerate(reader.pages):
                    text = page.extract_text()
                    if text and len(text.strip()) > 50: # Ch·ªâ l·∫•y trang c√≥ n·ªôi dung ƒë√°ng k·ªÉ
                        docs.append(Document(
                            page_content=text,
                            metadata={"source": filename, "page": i + 1}
                        ))
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file {pdf_path}. B·ªè qua.")
        
        return docs

    def get_vector_store(self, force_rebuild=False):
        """T·∫£i ho·∫∑c x√¢y d·ª±ng l·∫°i Vector Database."""
        # N·∫øu th∆∞ m·ª•c DB t·ªìn t·∫°i v√† kh√¥ng b·ªã √©p build l·∫°i, th√¨ load l√™n
        if os.path.exists(AppConfig.VECTOR_DB_PATH) and not force_rebuild:
            try:
                return FAISS.load_local(
                    AppConfig.VECTOR_DB_PATH, 
                    self.embeddings, 
                    allow_dangerous_deserialization=True
                )
            except Exception:
                pass # N·∫øu l·ªói load th√¨ build l·∫°i t·ª´ ƒë·∫ßu

        # Build m·ªõi (Ch·∫°y khi x√≥a folder ho·∫∑c l·∫ßn ƒë·∫ßu ti√™n)
        docs = self.load_documents()
        if not docs:
            return None
            
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=AppConfig.CHUNK_SIZE,
            chunk_overlap=AppConfig.CHUNK_OVERLAP
        )
        splits = splitter.split_documents(docs)
        
        vector_db = FAISS.from_documents(splits, self.embeddings)
        vector_db.save_local(AppConfig.VECTOR_DB_PATH)
        return vector_db

# ==============================================================================
# 5. C√ÅC H√ÄM H·ªñ TR·ª¢ (UTILITIES)
# ==============================================================================

def translate_query(text: str, translator) -> str:
    """D·ªãch c√¢u h·ªèi t·ª´ Vi·ªát sang Anh ƒë·ªÉ RAG ho·∫°t ƒë·ªông t·ªët h∆°n v·ªõi t√†i li·ªáu ti·∫øng Anh."""
    if not translator:
        return text
    try:
        # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh l·ªói model
        result = translator(text[:512]) 
        return result[0]['translation_text']
    except Exception:
        return text

def retrieve_info(vector_db, query: str) -> Tuple[str, List[str]]:
    """T√¨m ki·∫øm th√¥ng tin li√™n quan trong Vector DB."""
    if not vector_db:
        return "", []
    
    try:
        results = vector_db.similarity_search(query, k=AppConfig.TOP_K_RETRIEVAL)
        context_str = ""
        sources_list = []
        
        for doc in results:
            context_str += f"---\nN·ªôi dung: {doc.page_content}\n"
            sources_list.append(f"üìÑ {doc.metadata['source']} (Trang {doc.metadata['page']})")
            
        return context_str, list(set(sources_list)) # Remove duplicates
    except Exception as e:
        return "", []

def generate_response_stream(client, context, question):
    """T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ LLM (Streaming)."""
    # Prompt ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ ch√∫ √Ω ƒë·∫øn l·ªõp h·ªçc
    system_prompt = f"""
    B·∫°n l√† KTC Assistant, m·ªôt tr·ª£ l√Ω gi√°o d·ª•c chuy√™n nghi·ªáp.
    
    QUY T·∫ÆC QUAN TR·ªåNG:
    1. N·∫øu c√¢u h·ªèi c√≥ nh·∫Øc ƒë·∫øn L·ªöP c·ª• th·ªÉ (V√≠ d·ª•: Tin 10, Tin 11, Tin 12), h√£y ∆ØU TI√äN t√¨m ki·∫øm v√† tr·∫£ l·ªùi th√¥ng tin t·ª´ t√†i li·ªáu c·ªßa l·ªõp ƒë√≥ trong [TH√îNG TIN ƒê∆Ø·ª¢C CUNG C·∫§P].
    2. N·∫øu th√¥ng tin c·ªßa l·ªõp ƒë∆∞·ª£c h·ªèi kh√¥ng c√≥ trong d·ªØ li·ªáu, h√£y n√≥i r√µ l√† ch∆∞a c√≥ th√¥ng tin c·ªßa l·ªõp ƒë√≥.
    3. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, gi·ªçng vƒÉn s∆∞ ph·∫°m, d·ªÖ hi·ªÉu.
    4. Tr√¨nh b√†y ƒë·∫πp m·∫Øt (d√πng Markdown, bullet points).

    [TH√îNG TIN ƒê∆Ø·ª¢C CUNG C·∫§P T·ª™ T√ÄI LI·ªÜU]:
    {context}
    """
    
    try:
        stream = client.chat.completions.create(
            model=AppConfig.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            stream=True,
            temperature=0.3
        )
        return stream
    except Exception as e:
        return f"L·ªói k·∫øt n·ªëi AI: {str(e)}"

# ==============================================================================
# 6. MAIN APP LOOP (LOGO NH·ªé & C√ÇN ƒê·ªêI)
# ==============================================================================

def main():
    inject_custom_css()
    
    # --- C·∫•u h√¨nh Sidebar ---
    with st.sidebar:
        # 1. X·ª≠ l√Ω Logo: Chia c·ªôt ƒë·ªÉ logo nh·ªè l·∫°i v√† n·∫±m gi·ªØa (T·ª∑ l·ªá 1-3-1)
        if os.path.exists("LOGO.jpg"):
            c1, c2, c3 = st.columns([1, 3, 1]) 
            with c2: 
                st.image("LOGO.jpg", use_container_width=True)
        else:
            st.title("ü§ñ")
        
        st.write("") # T·∫°o kho·∫£ng c√°ch nh·ªè x√≠u d∆∞·ªõi logo
        
        # 2. Th√¥ng tin d·ª± √°n (G·ªçn g√†ng)
        st.markdown("""
        <div class="project-info">
            <div style="text-align: center; font-weight: bold; margin-bottom: 5px; color: #0f4c81;">
                üèÜ S·∫¢N PH·∫®M D·ª∞ THI<br>KHKT C·∫§P TR∆Ø·ªúNG
            </div>
            <b>THCS & THPT Ph·∫°m Ki·ªát</b><br>
            T√°c gi·∫£: B√πi T√° T√πng & Cao S·ªπ B·∫£o Chung<br>
            GVHD: Th·∫ßy Khanh
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # N√∫t x√≥a l·ªãch s·ª≠
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True, key="btn_clear"):
            st.session_state.messages = []
            st.rerun()

    # --- Giao di·ªán ch√≠nh (B√™n ph·∫£i) ---
    st.markdown("""
    <div class="main-header">
        <h1>üéì TR·ª¢ L√ù ·∫¢O KTC AI</h1>
        <p>H·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c Tin h·ªçc & Nghi√™n c·ª©u khoa h·ªçc</p>
    </div>
    """, unsafe_allow_html=True)

    # Kh·ªüi t·∫°o Session State
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ch√†o b·∫°n! M√¨nh l√† **KTC AI**. M√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b√†i h·ªçc h√¥m nay? üßë‚Äçüíª"}
        ]
    
    # Load Resources
    groq_client = load_groq_client()
    translator = load_translator()
    
    if "vector_db" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c..."):
            # Logic: N·∫øu kh√¥ng t√¨m th·∫•y vector_db tr√™n ƒëƒ©a th√¨ t·ª± ƒë·ªông build l·∫°i
            kb = KnowledgeBaseManager()
            st.session_state.vector_db = kb.get_vector_store()

    if not groq_client:
        st.error("‚ö†Ô∏è L·ªói API Key: Vui l√≤ng ki·ªÉm tra file c·∫•u h√¨nh secrets.")
        st.stop()

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # X·ª≠ l√Ω Chat Input
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(prompt)

        with st.chat_message("assistant", avatar="ü§ñ"):
            message_placeholder = st.empty()
            
            search_query = prompt
            if translator:
                translated = translate_query(prompt, translator)
                if translated and translated != prompt:
                    search_query = translated

            context_text, sources = retrieve_info(st.session_state.vector_db, search_query)
            
            if not context_text:
                context_text = "Kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ trong t√†i li·ªáu. Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung."

            stream = generate_response_stream(groq_client, context_text, prompt)
            
            full_response = ""
            if isinstance(stream, str):
                full_response = stream
                message_placeholder.markdown(full_response)
            else:
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response + "‚ñå")
                message_placeholder.markdown(full_response)

            if sources:
                with st.expander("üìö T√†i li·ªáu tham kh·∫£o & Minh ch·ª©ng"):
                    for src in sources:
                        st.markdown(f"- {src}")
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()