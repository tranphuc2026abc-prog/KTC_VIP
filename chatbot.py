# ============================================
# KTC Assistant ‚Äì RAG Chatbot (UI t·ªëi ∆∞u m·ªõi)
# ============================================

import os
import glob
from typing import List, Tuple, Any, Generator
import streamlit as st

# --------- Import ki·ªÉm so√°t l·ªói -----------
try:
    from pypdf import PdfReader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
    from groq import Groq
except ImportError as e:
    st.error(f"‚ùå Thi·∫øu th∆∞ vi·ªán: {e}. H√£y ch·∫°y: pip install -r requirements.txt")
    st.stop()

# --------- C√†i ƒë·∫∑t chung -----------
st.set_page_config(page_title="KTC Assistant", page_icon="ü§ñ", layout="wide")

class AppConfig:
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB = "faiss_db_index"
    LOGO_PATH = "LOGO.jpg"

    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    TRANSLATION_MODEL = "Helsinki-NLP/opus-mt-vi-en"
    LLM_MODEL = "llama-3.1-8b-instant"

    CHUNK = 1000
    OVERLAP = 200
    TOP_K = 5


# ============================================
#  UI / CSS ‚Äì giao di·ªán theo phong c√°ch ChatGPT
# ============================================
def inject_css():
    st.markdown("""
    <style>

    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');

    html, body, .css-18e3th9, .css-1d391kg {
        font-family: 'Inter', sans-serif !important;
    }

    /* Header */
    .main-header {
        background: linear-gradient(90deg, #1640F0, #4CB0FF);
        padding: 20px;
        border-radius: 14px;
        color: white;
        margin-bottom: 18px;
        box-shadow: 0px 4px 14px rgba(0,0,0,0.12);
    }

    /* Chat bubble style */
    .chat-bubble-user {
        background: #DCF2FF;
        padding: 12px 16px;
        border-radius: 12px;
        margin-bottom: 8px;
        width: fit-content;
        max-width: 80%;
        animation: fadeIn 0.25s ease;
    }

    .chat-bubble-assistant {
        background: #FFFFFF;
        padding: 12px 16px;
        border-radius: 12px;
        margin-bottom: 8px;
        width: fit-content;
        max-width: 80%;
        border-left: 4px solid #4CB0FF;
        box-shadow: 0px 2px 6px rgba(0,0,0,0.05);
        animation: fadeIn 0.25s ease;
    }

    /* Animation */
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(4px); }
        to { opacity: 1; transform: translateY(0); }
    }

    /* Sidebar card */
    .sb-card {
        background: white;
        padding: 14px;
        border-radius: 10px;
        border-left: 5px solid #1640F0;
    }

    /* Chat input area */
    .stChatInput > div > div textarea {
        border-radius: 10px !important;
        padding: 14px !important;
        font-size: 16px !important;
    }

    </style>
    """, unsafe_allow_html=True)


# ============================================
#  T·∫£i m√¥ h√¨nh (cache)
# ============================================

@st.cache_resource
def get_client():
    key = st.secrets.get("GROQ_API_KEY")
    return Groq(api_key=key) if key else None

@st.cache_resource
def embed_model():
    return HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)

@st.cache_resource
# Thay th·∫ø h√†m load_translator hi·ªán t·∫°i b·∫±ng phi√™n b·∫£n an to√†n (kh√¥ng crash khi offline)
@st.cache_resource(show_spinner=False)
def load_translator():
    """
    Th·ª≠ t·∫£i b·ªô d·ªãch. N·∫øu th·∫•t b·∫°i (v√≠ d·ª• m√¥i tr∆∞·ªùng kh√¥ng c√≥ internet / model kh√¥ng c√≥ s·∫µn),
    tr·∫£ v·ªÅ None ƒë·ªÉ app ti·∫øp t·ª•c ho·∫°t ƒë·ªông.
    """
    try:
        # N·∫øu ng∆∞·ªùi d√πng ƒë·∫∑t TRANSLATION_MODEL = None th√¨ skip
        if not AppConfig.TRANSLATION_MODEL:
            return None
        tokenizer = AutoTokenizer.from_pretrained(AppConfig.TRANSLATION_MODEL)
        model = AutoModelForSeq2SeqLM.from_pretrained(AppConfig.TRANSLATION_MODEL)
        # t·∫°o pipeline nh∆∞ng kh√¥ng ƒë·∫∑t src/tgt (v√¨ m·ªôt s·ªë phi√™n b·∫£n transformers ko ch·∫•p nh·∫≠n)
        return pipeline("translation", model=model, tokenizer=tokenizer)
    except Exception as e:
        # Ghi log/hi·ªán warning (kh√¥ng d·ª´ng app)
        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i model d·ªãch ({AppConfig.TRANSLATION_MODEL}): {e}. Ti·∫øp t·ª•c kh√¥ng d√πng translator.")
        return None


def translate_query(text: str, translator) -> str:
    """
    N·∫øu c√≥ translator th√¨ th·ª≠ d·ªãch (c·∫Øt gi·ªõi h·∫°n chars ƒë·ªÉ tr√°nh l·ªói v·ªõi model l·ªõn).
    N·∫øu translator l√† None ho·∫∑c qu√° tr√¨nh d·ªãch l·ªói -> tr·∫£ l·∫°i text g·ªëc.
    """
    if not translator or not text:
        return text
    try:
        # M·ªôt s·ªë pipeline tr·∫£ list, m·ªôt s·ªë tr·∫£ dict, handle c·∫£ hai
        out = translator(text[:500])  # gi·ªõi h·∫°n 500 k√Ω t·ª± cho an to√†n
        if isinstance(out, list) and len(out) > 0:
            first = out[0]
            if isinstance(first, dict):
                return first.get("translation_text") or first.get("text") or text
            elif isinstance(first, str):
                return first
        if isinstance(out, dict):
            return out.get("translation_text") or out.get("text") or text
        return text
    except Exception as e:
        # N·∫øu d·ªãch l·ªói, kh√¥ng d·ª´ng app
        st.warning(f"‚ö†Ô∏è L·ªói khi d·ªãch (b·ªè qua): {e}")
        return text

@st.cache_data
def read_pdfs(folder: str):
    docs = []
    if not os.path.exists(folder):
        return docs

    for path in sorted(glob.glob(folder + "/*.pdf")):
        reader = PdfReader(path)
        name = os.path.basename(path)
        for i, p in enumerate(reader.pages):
            text = p.extract_text() or ""
            if text.strip():
                docs.append(Document(page_content=text, metadata={"source": name, "page": i+1}))
    return docs

@st.cache_resource
def build_db(docs, embeddings):
    if not docs:
        return None

    if os.path.exists(AppConfig.VECTOR_DB):
        try:
            return FAISS.load_local(AppConfig.VECTOR_DB, embeddings, allow_dangerous_deserialization=True)
        except:
            pass

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=AppConfig.CHUNK,
        chunk_overlap=AppConfig.OVERLAP
    )
    chunks = splitter.split_documents(docs)

    db = FAISS.from_documents(chunks, embeddings)
    db.save_local(AppConfig.VECTOR_DB)
    return db


# ============================================
#  X·ª≠ l√Ω truy v·∫•n
# ============================================

def translate(text, translator):
    try:
        return translator(text[:500])[0]["translation_text"]
    except:
        return text

def retrieve(db, query):
    if not db:
        return "", []

    results = db.similarity_search(query, k=AppConfig.TOP_K)
    parts, src_list = [], []

    for d in results:
        parts.append(f"[Ngu·ªìn: {d.metadata['source']} ‚Äì Trang {d.metadata['page']}]\n{d.page_content}")
        src_list.append(f"{d.metadata['source']} (Trang {d.metadata['page']})")

    uniq = list(dict.fromkeys(src_list))
    return "\n\n".join(parts), uniq


def stream_answer(client, ctx, question):
    system = f"""
B·∫°n l√† KTC Assistant ‚Äì tr·ª£ l√Ω gi√°o d·ª•c.
∆Øu ti√™n d√πng d·ªØ li·ªáu t·ª´ CONTEXT, sau ƒë√≥ m·ªõi t·ªõi ki·∫øn th·ª©c n·ªÅn.

[CONTEXT]:
{ctx}
"""
    return client.chat.completions.create(
        model=AppConfig.LLM_MODEL,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": question}
        ],
        stream=True,
        temperature=0.2
    )


def safe_stream(gen):
    for chunk in gen:
        try:
            delta = chunk["choices"][0]["delta"].get("content", "")
            if delta:
                yield delta
        except:
            pass


# ============================================
#  MAIN UI
# ============================================
def main():
    inject_css()

    # Sidebar
    with st.sidebar:
        if os.path.exists(AppConfig.LOGO_PATH):
            st.image(AppConfig.LOGO_PATH, use_container_width=True)
        st.markdown("<div class='sb-card'><b>Kho tri th·ª©c:</b> 6 file PDF trong th∆∞ m·ª•c <code>PDF_KNOWLEDGE</code></div>", unsafe_allow_html=True)

    # Header
    st.markdown("""
        <div class="main-header">
            <h2 style="margin:0;">ü§ñ KTC Assistant ‚Äì RAG Chatbot</h2>
            <div style="opacity:0.9">Tr√≠ tu·ªá nh√¢n t·∫°o h·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c t·ª´ PDF</div>
        </div>
    """, unsafe_allow_html=True)

    # Session state
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Load resources
    client = get_client()
    translator = load_translator()
    embeddings = embed_model()

    docs = read_pdfs(AppConfig.PDF_DIR)
    db = build_db(docs, embeddings)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for m in st.session_state.messages:
        role = m["role"]
        bubble = "chat-bubble-user" if role == "user" else "chat-bubble-assistant"
        st.markdown(f"<div class='{bubble}'>{m['content']}</div>", unsafe_allow_html=True)

    # Nh·∫≠p c√¢u h·ªèi
    question = st.chat_input("Nh·∫≠p c√¢u h·ªèi...")

    if question:

        # L∆∞u c√¢u h·ªèi
        st.session_state.messages.append({"role": "user", "content": question})
        st.markdown(f"<div class='chat-bubble-user'>{question}</div>", unsafe_allow_html=True)

        # D·ªãch truy v·∫•n tr∆∞·ªõc khi search
        q_trans = translate(question, translator)

        ctx, src = retrieve(db, q_trans)

        # Stream c√¢u tr·∫£ l·ªùi
        with st.chat_message("assistant"):
            placeholder = st.empty()
            full = ""

            stream = stream_answer(client, ctx, question)

            for t in safe_stream(stream):
                full += t
                placeholder.markdown(f"<div class='chat-bubble-assistant'>{full}‚ñå</div>", unsafe_allow_html=True)

            placeholder.markdown(f"<div class='chat-bubble-assistant'>{full}</div>", unsafe_allow_html=True)

        st.session_state.messages.append({"role":"assistant","content":full})

        if src:
            with st.expander("üìö Ngu·ªìn tham kh·∫£o"):
                for s in src:
                    st.write("- " + s)


if __name__ == "__main__":
    main()
